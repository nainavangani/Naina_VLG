{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366549e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4afeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aefa2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-2.033875</td>\n",
       "      <td>0.978446</td>\n",
       "      <td>-0.142131</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-1.470684</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>-0.196530</td>\n",
       "      <td>-0.125239</td>\n",
       "      <td>-0.452284</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111266</td>\n",
       "      <td>0.716084</td>\n",
       "      <td>0.060039</td>\n",
       "      <td>0.301279</td>\n",
       "      <td>-1.174846</td>\n",
       "      <td>-1.076498</td>\n",
       "      <td>-0.069452</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-2.179176</td>\n",
       "      <td>0.558003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.348835</td>\n",
       "      <td>0.294815</td>\n",
       "      <td>-0.557577</td>\n",
       "      <td>-2.020773</td>\n",
       "      <td>-1.234715</td>\n",
       "      <td>1.633930</td>\n",
       "      <td>-1.680658</td>\n",
       "      <td>-0.358146</td>\n",
       "      <td>0.166122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735240</td>\n",
       "      <td>0.829781</td>\n",
       "      <td>1.521941</td>\n",
       "      <td>1.347946</td>\n",
       "      <td>0.754505</td>\n",
       "      <td>1.330642</td>\n",
       "      <td>-0.754453</td>\n",
       "      <td>0.582956</td>\n",
       "      <td>0.252671</td>\n",
       "      <td>1.495870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.113248</td>\n",
       "      <td>-0.607726</td>\n",
       "      <td>-0.947791</td>\n",
       "      <td>0.830851</td>\n",
       "      <td>0.998291</td>\n",
       "      <td>0.498321</td>\n",
       "      <td>-1.493958</td>\n",
       "      <td>0.789572</td>\n",
       "      <td>-1.311018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104698</td>\n",
       "      <td>0.616189</td>\n",
       "      <td>-1.035953</td>\n",
       "      <td>2.111387</td>\n",
       "      <td>-0.984415</td>\n",
       "      <td>1.148076</td>\n",
       "      <td>-1.433554</td>\n",
       "      <td>0.243372</td>\n",
       "      <td>0.170083</td>\n",
       "      <td>1.274795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.223321</td>\n",
       "      <td>-0.479048</td>\n",
       "      <td>-1.925789</td>\n",
       "      <td>1.680377</td>\n",
       "      <td>0.021840</td>\n",
       "      <td>-1.453307</td>\n",
       "      <td>0.605559</td>\n",
       "      <td>-0.019024</td>\n",
       "      <td>1.065448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360237</td>\n",
       "      <td>-1.957863</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>1.505329</td>\n",
       "      <td>0.660290</td>\n",
       "      <td>-1.769443</td>\n",
       "      <td>-0.547756</td>\n",
       "      <td>-0.568122</td>\n",
       "      <td>0.244645</td>\n",
       "      <td>0.982116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.160109</td>\n",
       "      <td>0.422684</td>\n",
       "      <td>-0.308029</td>\n",
       "      <td>0.227744</td>\n",
       "      <td>0.432854</td>\n",
       "      <td>0.608348</td>\n",
       "      <td>0.193832</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>-0.538868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416629</td>\n",
       "      <td>1.441766</td>\n",
       "      <td>0.212572</td>\n",
       "      <td>-0.994721</td>\n",
       "      <td>1.143999</td>\n",
       "      <td>-2.166923</td>\n",
       "      <td>-1.199248</td>\n",
       "      <td>-1.028636</td>\n",
       "      <td>0.752791</td>\n",
       "      <td>0.317169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels       f_0       f_1       f_2       f_3       f_4       f_5  \\\n",
       "0       0 -2.033875  0.978446 -0.142131 -0.177117 -1.470684  1.669562   \n",
       "1       1 -0.348835  0.294815 -0.557577 -2.020773 -1.234715  1.633930   \n",
       "2       1  0.113248 -0.607726 -0.947791  0.830851  0.998291  0.498321   \n",
       "3       0  1.223321 -0.479048 -1.925789  1.680377  0.021840 -1.453307   \n",
       "4       0  0.160109  0.422684 -0.308029  0.227744  0.432854  0.608348   \n",
       "\n",
       "        f_6       f_7       f_8  ...    f_1190    f_1191    f_1192    f_1193  \\\n",
       "0 -0.196530 -0.125239 -0.452284  ... -1.111266  0.716084  0.060039  0.301279   \n",
       "1 -1.680658 -0.358146  0.166122  ...  0.735240  0.829781  1.521941  1.347946   \n",
       "2 -1.493958  0.789572 -1.311018  ...  0.104698  0.616189 -1.035953  2.111387   \n",
       "3  0.605559 -0.019024  1.065448  ...  0.360237 -1.957863 -0.123384  1.505329   \n",
       "4  0.193832  1.035091 -0.538868  ...  0.416629  1.441766  0.212572 -0.994721   \n",
       "\n",
       "     f_1194    f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0 -1.174846 -1.076498 -0.069452 -0.604012 -2.179176  0.558003  \n",
       "1  0.754505  1.330642 -0.754453  0.582956  0.252671  1.495870  \n",
       "2 -0.984415  1.148076 -1.433554  0.243372  0.170083  1.274795  \n",
       "3  0.660290 -1.769443 -0.547756 -0.568122  0.244645  0.982116  \n",
       "4  1.143999 -2.166923 -1.199248 -1.028636  0.752791  0.317169  \n",
       "\n",
       "[5 rows x 1201 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96eb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebbe1a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f_0</th>\n",
       "      <th>f_1</th>\n",
       "      <th>f_2</th>\n",
       "      <th>f_3</th>\n",
       "      <th>f_4</th>\n",
       "      <th>f_5</th>\n",
       "      <th>f_6</th>\n",
       "      <th>f_7</th>\n",
       "      <th>f_8</th>\n",
       "      <th>...</th>\n",
       "      <th>f_1190</th>\n",
       "      <th>f_1191</th>\n",
       "      <th>f_1192</th>\n",
       "      <th>f_1193</th>\n",
       "      <th>f_1194</th>\n",
       "      <th>f_1195</th>\n",
       "      <th>f_1196</th>\n",
       "      <th>f_1197</th>\n",
       "      <th>f_1198</th>\n",
       "      <th>f_1199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-3.388242</td>\n",
       "      <td>0.868285</td>\n",
       "      <td>-0.427619</td>\n",
       "      <td>-0.678964</td>\n",
       "      <td>-1.625735</td>\n",
       "      <td>0.262761</td>\n",
       "      <td>1.243040</td>\n",
       "      <td>1.537751</td>\n",
       "      <td>-0.352028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.776403</td>\n",
       "      <td>-0.662884</td>\n",
       "      <td>-0.257091</td>\n",
       "      <td>-1.168413</td>\n",
       "      <td>0.223260</td>\n",
       "      <td>-0.482520</td>\n",
       "      <td>-0.085453</td>\n",
       "      <td>-0.382265</td>\n",
       "      <td>-0.539349</td>\n",
       "      <td>-1.682404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.496920</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.989040</td>\n",
       "      <td>0.451422</td>\n",
       "      <td>0.513516</td>\n",
       "      <td>-0.099658</td>\n",
       "      <td>-1.124326</td>\n",
       "      <td>0.729430</td>\n",
       "      <td>-0.216224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379635</td>\n",
       "      <td>-1.760084</td>\n",
       "      <td>1.125450</td>\n",
       "      <td>-0.328047</td>\n",
       "      <td>-0.880305</td>\n",
       "      <td>-1.257607</td>\n",
       "      <td>0.964312</td>\n",
       "      <td>2.021104</td>\n",
       "      <td>0.655021</td>\n",
       "      <td>-0.423029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.128369</td>\n",
       "      <td>-0.537951</td>\n",
       "      <td>2.544358</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>-1.904994</td>\n",
       "      <td>0.776961</td>\n",
       "      <td>-0.495768</td>\n",
       "      <td>0.060111</td>\n",
       "      <td>-1.418468</td>\n",
       "      <td>...</td>\n",
       "      <td>1.165254</td>\n",
       "      <td>-1.373589</td>\n",
       "      <td>-0.483701</td>\n",
       "      <td>-0.964782</td>\n",
       "      <td>-0.869555</td>\n",
       "      <td>0.066040</td>\n",
       "      <td>-0.444567</td>\n",
       "      <td>-0.531935</td>\n",
       "      <td>-0.878660</td>\n",
       "      <td>1.099488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.051253</td>\n",
       "      <td>1.746814</td>\n",
       "      <td>0.681177</td>\n",
       "      <td>1.844524</td>\n",
       "      <td>-0.327977</td>\n",
       "      <td>1.226839</td>\n",
       "      <td>-0.085519</td>\n",
       "      <td>0.379008</td>\n",
       "      <td>-1.003667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442288</td>\n",
       "      <td>-2.794472</td>\n",
       "      <td>-0.763468</td>\n",
       "      <td>-0.789832</td>\n",
       "      <td>-0.113209</td>\n",
       "      <td>-2.703150</td>\n",
       "      <td>-2.058728</td>\n",
       "      <td>1.070627</td>\n",
       "      <td>-0.458045</td>\n",
       "      <td>-0.435825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.423209</td>\n",
       "      <td>-0.983594</td>\n",
       "      <td>-1.694170</td>\n",
       "      <td>1.197507</td>\n",
       "      <td>1.044211</td>\n",
       "      <td>0.518777</td>\n",
       "      <td>-0.298612</td>\n",
       "      <td>-0.365174</td>\n",
       "      <td>0.738447</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.624450</td>\n",
       "      <td>-3.200223</td>\n",
       "      <td>0.711422</td>\n",
       "      <td>-0.190394</td>\n",
       "      <td>0.337224</td>\n",
       "      <td>-1.656639</td>\n",
       "      <td>0.707360</td>\n",
       "      <td>-0.562290</td>\n",
       "      <td>1.471181</td>\n",
       "      <td>-0.192000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       f_0       f_1       f_2       f_3       f_4       f_5       f_6  \\\n",
       "0   1 -3.388242  0.868285 -0.427619 -0.678964 -1.625735  0.262761  1.243040   \n",
       "1   2 -0.496920  0.952381  0.989040  0.451422  0.513516 -0.099658 -1.124326   \n",
       "2   3  1.128369 -0.537951  2.544358  1.165254 -1.904994  0.776961 -0.495768   \n",
       "3   4  0.051253  1.746814  0.681177  1.844524 -0.327977  1.226839 -0.085519   \n",
       "4   5  1.423209 -0.983594 -1.694170  1.197507  1.044211  0.518777 -0.298612   \n",
       "\n",
       "        f_7       f_8  ...    f_1190    f_1191    f_1192    f_1193    f_1194  \\\n",
       "0  1.537751 -0.352028  ... -0.776403 -0.662884 -0.257091 -1.168413  0.223260   \n",
       "1  0.729430 -0.216224  ...  0.379635 -1.760084  1.125450 -0.328047 -0.880305   \n",
       "2  0.060111 -1.418468  ...  1.165254 -1.373589 -0.483701 -0.964782 -0.869555   \n",
       "3  0.379008 -1.003667  ... -0.442288 -2.794472 -0.763468 -0.789832 -0.113209   \n",
       "4 -0.365174  0.738447  ... -2.624450 -3.200223  0.711422 -0.190394  0.337224   \n",
       "\n",
       "     f_1195    f_1196    f_1197    f_1198    f_1199  \n",
       "0 -0.482520 -0.085453 -0.382265 -0.539349 -1.682404  \n",
       "1 -1.257607  0.964312  2.021104  0.655021 -0.423029  \n",
       "2  0.066040 -0.444567 -0.531935 -0.878660  1.099488  \n",
       "3 -2.703150 -2.058728  1.070627 -0.458045 -0.435825  \n",
       "4 -1.656639  0.707360 -0.562290  1.471181 -0.192000  \n",
       "\n",
       "[5 rows x 1201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b788118d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 1201)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82d22b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2250, 1201)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d6f7d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels    0\n",
       "f_0       0\n",
       "f_1       0\n",
       "f_2       0\n",
       "f_3       0\n",
       "         ..\n",
       "f_1195    0\n",
       "f_1196    0\n",
       "f_1197    0\n",
       "f_1198    0\n",
       "f_1199    0\n",
       "Length: 1201, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0e8f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels       2\n",
       "f_0       5242\n",
       "f_1       5244\n",
       "f_2       5245\n",
       "f_3       5247\n",
       "          ... \n",
       "f_1195    5242\n",
       "f_1196    5247\n",
       "f_1197    5244\n",
       "f_1198    5243\n",
       "f_1199    5242\n",
       "Length: 1201, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd7a166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5250 entries, 0 to 5249\n",
      "Columns: 1201 entries, labels to f_1199\n",
      "dtypes: float64(1200), int64(1)\n",
      "memory usage: 48.1 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1ff4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('labels' , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32ab6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f412a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d6a0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop('id' , axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6800fc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  labels\n",
       "0   1       0\n",
       "1   2       1\n",
       "2   3       0\n",
       "3   4       1\n",
       "4   5       0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_csv(\"solution_format.csv\")\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bcb395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open the input CSV file\n",
    "with open('solution_format.csv', 'r') as input_file:\n",
    "    # Read the input CSV data\n",
    "    reader = csv.reader(input_file)\n",
    "    rows = list(reader)\n",
    "\n",
    "# Determine the index of the column you want to delete\n",
    "column_index = 1  # Replace with the actual index of the column to delete\n",
    "\n",
    "# Remove the column from each row\n",
    "for row in rows:\n",
    "    del row[column_index]\n",
    "\n",
    "# Open the output CSV file\n",
    "with open('output.csv', 'w', newline='') as output_file:\n",
    "    # Write the modified data to the output CSV file\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1c2ace4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id\n",
       "0   1\n",
       "1   2\n",
       "2   3\n",
       "3   4\n",
       "4   5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.read_csv('output.csv')\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec86faab",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b798256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6432160804020101\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the F1 score\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e93a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm.fit(x,y)\n",
    "ans = svm.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806ffe96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03db41d",
   "metadata": {},
   "source": [
    "SVM with Polynomial of degree 2 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "054d48a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 0.1, 'gamma': 0.1}\n",
      "F1 score: 0.6224489795918368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Perform feature scaling on the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='poly', degree=2)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, scoring='f1', cv=5)\n",
    "\n",
    "# Fit the model with training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the F1 score of the model\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84751305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(x,y)\n",
    "ans = svm.predict(test)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d48d76",
   "metadata": {},
   "source": [
    "SVM with rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4482e3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 0.001}\n",
      "F1 score: 0.6839622641509433\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Perform feature scaling on the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_classifier = SVC(kernel='rbf')\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 0.01, 0.001],\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, scoring='f1', cv=5)\n",
    "\n",
    "# Fit the model with training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate the F1 score of the model\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad47622d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(x,y)\n",
    "ans = best_model.predict(test)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054ee94d",
   "metadata": {},
   "source": [
    " Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c2b3164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "F1 score: 0.6205128205128205\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load your dataset\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the reduced parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 5],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, scoring='f1', cv=5)\n",
    "\n",
    "# Fit the model with training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a951884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.fit(x,y)\n",
    "ans = rf_classifier.predict(test)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da6ceb",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd3dd68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "F1 score: 0.7074829931972789\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, scoring='f1', cv=5)\n",
    "\n",
    "# Fit the model with training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e47f32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naina\\anaconda3\\lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(X,y)\n",
    "ansxgb = best_model.predict(test)\n",
    "ansxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eddb6f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the original CSV file\n",
    "original_file = 'output.csv'\n",
    "\n",
    "# Specify the path of the copy CSV file\n",
    "copy_file = 'output_copy.csv'\n",
    "\n",
    "# Make a copy of the original CSV file\n",
    "shutil.copyfile(original_file, copy_file)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the copied CSV file into a DataFrame\n",
    "d = pd.read_csv(copy_file)\n",
    "new_column = ansxgb\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "d['New Column'] = new_column\n",
    "# Save the updated copy DataFrame as a new CSV file\n",
    "d.to_csv(copy_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04d6713f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>New Column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  New Column\n",
       "0   1           0\n",
       "1   2           0\n",
       "2   3           0\n",
       "3   4           0\n",
       "4   5           0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pd.read_csv('output_copy.csv')\n",
    "b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f443ba1",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d576162c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6936\n",
      "Epoch 2/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.4295 - accuracy: 0.7986\n",
      "Epoch 3/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3666 - accuracy: 0.8236\n",
      "Epoch 4/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3567 - accuracy: 0.8300\n",
      "Epoch 5/20\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.3334 - accuracy: 0.8417\n",
      "Epoch 6/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3223 - accuracy: 0.8483\n",
      "Epoch 7/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3115 - accuracy: 0.8531\n",
      "Epoch 8/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3032 - accuracy: 0.8581\n",
      "Epoch 9/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8576\n",
      "Epoch 10/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2871 - accuracy: 0.8669\n",
      "Epoch 11/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2813 - accuracy: 0.8702\n",
      "Epoch 12/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8745\n",
      "Epoch 13/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8819\n",
      "Epoch 14/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.8814\n",
      "Epoch 15/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8898\n",
      "Epoch 16/20\n",
      "132/132 [==============================] - 1s 6ms/step - loss: 0.2401 - accuracy: 0.8948\n",
      "Epoch 17/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.8905\n",
      "Epoch 18/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2230 - accuracy: 0.9114\n",
      "Epoch 19/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2275 - accuracy: 0.8962\n",
      "Epoch 20/20\n",
      "132/132 [==============================] - 1s 5ms/step - loss: 0.2247 - accuracy: 0.9014\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "F1 score: 0.67420814479638\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Extract the feature matrix X and the label array y\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Reshape the data into the appropriate format for CNN input\n",
    "X = X.reshape(-1, 20, 20, 3)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CNN architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='leaky_relu', input_shape=(20, 20, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='leaky_relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=20, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "892e15ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step\n",
      "Predicted labels: [[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "test = test.values\n",
    "# Reshape the test data into the appropriate format for CNN input\n",
    "test = test.reshape(-1, 20, 20, 3)\n",
    "\n",
    "# Predict labels for the test dataset\n",
    "ans = model.predict(test)\n",
    "y_pred = (ans > 0.5).astype(int)\n",
    "\n",
    "# Print predicted labels\n",
    "print(\"Predicted labels:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f77d98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the original CSV file\n",
    "original_file = 'output.csv'\n",
    "\n",
    "# Specify the path of the copy CSV file\n",
    "copy_file = 'output_copy.csv'\n",
    "\n",
    "# Make a copy of the original CSV file\n",
    "shutil.copyfile(original_file, copy_file)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the copied CSV file into a DataFrame\n",
    "d = pd.read_csv(copy_file)\n",
    "new_column = y_pred\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "d['New Column'] = new_column\n",
    "# Save the updated copy DataFrame as a new CSV file\n",
    "d.to_csv(copy_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17402a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7c1372",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edd983b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "132/132 [==============================] - 1s 2ms/step - loss: 0.3604 - accuracy: 0.8298\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9483\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9998\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 9.2146e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 6.2962e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 4.6118e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 3.5174e-04 - accuracy: 1.0000\n",
      "33/33 [==============================] - 0s 893us/step\n",
      "F1 score: 0.6653771760154739\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the ANN model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(1200,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "165b291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 983us/step\n"
     ]
    }
   ],
   "source": [
    "s = test\n",
    "s = scaler.transform(s)\n",
    "y_predd_probs = model.predict(s)\n",
    "y_predd = (y_predd_probs > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e273afeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4998027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the original CSV file\n",
    "original_file = 'output.csv'\n",
    "\n",
    "# Specify the path of the copy CSV file\n",
    "copy_file = 'output_copy.csv'\n",
    "\n",
    "# Make a copy of the original CSV file\n",
    "shutil.copyfile(original_file, copy_file)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the copied CSV file into a DataFrame\n",
    "d = pd.read_csv(copy_file)\n",
    "new_column = y_predd\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "d['New Column'] = new_column\n",
    "# Save the updated copy DataFrame as a new CSV file\n",
    "d.to_csv(copy_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a878d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8b1abe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "132/132 [==============================] - 5s 31ms/step - loss: 0.4214 - accuracy: 0.7952\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 4s 31ms/step - loss: 0.3060 - accuracy: 0.8605\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 4s 32ms/step - loss: 0.2484 - accuracy: 0.8893\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 4s 34ms/step - loss: 0.1927 - accuracy: 0.9167\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 4s 33ms/step - loss: 0.1516 - accuracy: 0.9357\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.1307 - accuracy: 0.9464\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 4s 33ms/step - loss: 0.0963 - accuracy: 0.9598\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 4s 34ms/step - loss: 0.0736 - accuracy: 0.9710\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 4s 32ms/step - loss: 0.0726 - accuracy: 0.9721\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 4s 34ms/step - loss: 0.0669 - accuracy: 0.9764\n",
      "33/33 [==============================] - 0s 10ms/step\n",
      "F1 score: 0.7262357414448669\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform data normalization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create the ANN model\n",
    "model2 = Sequential()\n",
    "model2.add(Conv1D(filters=32, kernel_size=2, strides = 1, activation='relu', input_shape=(1200,1)))\n",
    "model2.add(Conv1D(filters=64, kernel_size=3, strides = 2, activation='relu'))\n",
    "model2.add(MaxPooling1D(pool_size=2))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dropout(rate = 0.5))\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model2.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model2.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f252400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 1s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=scaler.transform(test)\n",
    "y_predd_probs = model2.predict(test)\n",
    "y_predd = (y_predd_probs >0.5).astype(int)\n",
    "y_predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8927f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the path of the original CSV file\n",
    "original_file = 'output.csv'\n",
    "\n",
    "# Specify the path of the copy CSV file\n",
    "copy_file = 'output_copy.csv'\n",
    "\n",
    "# Make a copy of the original CSV file\n",
    "shutil.copyfile(original_file, copy_file)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the copied CSV file into a DataFrame\n",
    "d = pd.read_csv(copy_file)\n",
    "new_column = y_predd\n",
    "\n",
    "# Add the new column to the DataFrame\n",
    "d['New Column'] = new_column\n",
    "# Save the updated copy DataFrame as a new CSV file\n",
    "d.to_csv(copy_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c6489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5eef278",
   "metadata": {},
   "source": [
    "VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10e65844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naina\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 562s 4s/step - loss: 0.6567 - accuracy: 0.7205\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 559s 4s/step - loss: 0.6045 - accuracy: 0.7298\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 546s 4s/step - loss: 0.5868 - accuracy: 0.7308\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 509s 4s/step - loss: 0.5836 - accuracy: 0.7306\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 509s 4s/step - loss: 0.5693 - accuracy: 0.7308\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 507s 4s/step - loss: 0.5677 - accuracy: 0.7301\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 531s 4s/step - loss: 0.5691 - accuracy: 0.7313\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 561s 4s/step - loss: 0.5627 - accuracy: 0.7306\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 559s 4s/step - loss: 0.5585 - accuracy: 0.7303\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 551s 4s/step - loss: 0.5564 - accuracy: 0.7308\n",
      "Epoch 1/10\n",
      "131/131 [==============================] - 807s 6s/step - loss: 0.5987 - accuracy: 0.7325\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 930s 7s/step - loss: 0.5874 - accuracy: 0.7313\n",
      "Epoch 3/10\n",
      "100/131 [=====================>........] - ETA: 3:42 - loss: 0.5635 - accuracy: 0.7334"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20132/2880236285.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Train the model with fine-tuning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;31m# Make predictions on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Extract the feature matrix X and the label array y\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Reshape the data into the appropriate format for CNN input\n",
    "X = X.reshape(-1, 20, 20, 3)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Input layer with the desired input shape\n",
    "input_layer = Input(shape=(20, 20, 3))\n",
    "\n",
    "# Resize the input images to match the required input shape of VGG16\n",
    "resized_input = tf.image.resize(input_layer, (224, 224))\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=resized_input)\n",
    "\n",
    "# Freeze the layers of the base model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add your own layers on top of the base model\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data augmentation\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), steps_per_epoch=len(X_train) // 32, epochs=10, verbose=1)\n",
    "\n",
    "# Unfreeze the last convolutional block of the base model for fine-tuning\n",
    "for layer in base_model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Lower the learning rate for fine-tuning\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "\n",
    "# Train the model with fine-tuning\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32), steps_per_epoch=len(X_train) // 32, epochs=10, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfcf65a",
   "metadata": {},
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3da64f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 196s 1s/step - loss: 0.7034 - accuracy: 0.7027\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 140s 1s/step - loss: 0.5781 - accuracy: 0.7294\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 142s 1s/step - loss: 0.5704 - accuracy: 0.7301\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 158s 1s/step - loss: 0.5700 - accuracy: 0.7298\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 161s 1s/step - loss: 0.5632 - accuracy: 0.7306\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 168s 1s/step - loss: 0.5627 - accuracy: 0.7320\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 168s 1s/step - loss: 0.5627 - accuracy: 0.7337\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 158s 1s/step - loss: 0.5593 - accuracy: 0.7294\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 163s 1s/step - loss: 0.5602 - accuracy: 0.7298\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 165s 1s/step - loss: 0.5592 - accuracy: 0.7303\n",
      "33/33 [==============================] - 7s 190ms/step\n",
      "F1 score: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Extract the feature matrix X and the label array y\n",
    "X = df.drop('labels', axis=1).values\n",
    "y = df['labels'].values\n",
    "\n",
    "# Resize the images to (32, 32) to meet the minimum input size of ResNet50\n",
    "X_resized = np.array([resize(img, (32, 32, 3)) for img in X])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an ImageDataGenerator for data augmentation and preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Load the pre-trained ResNet50 model without the top layers\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "\n",
    "# Create a new model on top of the pre-trained model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model using the data generator\n",
    "model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "          steps_per_epoch=len(X_train) // 32,\n",
    "          epochs=10,\n",
    "          verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97f0d31",
   "metadata": {},
   "source": [
    "MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b0e60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 101s 753ms/step - loss: 0.5975 - accuracy: 0.7049 - val_loss: 0.5694 - val_accuracy: 0.7438\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 91s 694ms/step - loss: 0.5813 - accuracy: 0.7306 - val_loss: 0.5691 - val_accuracy: 0.7438\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 92s 701ms/step - loss: 0.5804 - accuracy: 0.7301 - val_loss: 0.5704 - val_accuracy: 0.7438\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 91s 697ms/step - loss: 0.5789 - accuracy: 0.7306 - val_loss: 0.5632 - val_accuracy: 0.7438\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 90s 684ms/step - loss: 0.5756 - accuracy: 0.7310 - val_loss: 0.5609 - val_accuracy: 0.7438\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 91s 691ms/step - loss: 0.5766 - accuracy: 0.7303 - val_loss: 0.5596 - val_accuracy: 0.7438\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 90s 689ms/step - loss: 0.5736 - accuracy: 0.7306 - val_loss: 0.5636 - val_accuracy: 0.7438\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 89s 683ms/step - loss: 0.5718 - accuracy: 0.7313 - val_loss: 0.5580 - val_accuracy: 0.7438\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 90s 689ms/step - loss: 0.5716 - accuracy: 0.7301 - val_loss: 0.5544 - val_accuracy: 0.7438\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 89s 683ms/step - loss: 0.5710 - accuracy: 0.7298 - val_loss: 0.5529 - val_accuracy: 0.7438\n",
      "Epoch 1/10\n",
      "131/131 [==============================] - 115s 851ms/step - loss: 0.6460 - accuracy: 0.6960 - val_loss: 3.5091 - val_accuracy: 0.2562\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 110s 841ms/step - loss: 0.5742 - accuracy: 0.7109 - val_loss: 1.3357 - val_accuracy: 0.7438\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 101s 773ms/step - loss: 0.5561 - accuracy: 0.7246 - val_loss: 1.6471 - val_accuracy: 0.2638\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 102s 779ms/step - loss: 0.5614 - accuracy: 0.7236 - val_loss: 0.5178 - val_accuracy: 0.7505\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 99s 764ms/step - loss: 0.5681 - accuracy: 0.7169 - val_loss: 1.5862 - val_accuracy: 0.2733\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 101s 772ms/step - loss: 0.5391 - accuracy: 0.7315 - val_loss: 0.5287 - val_accuracy: 0.7629\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 102s 778ms/step - loss: 0.5357 - accuracy: 0.7310 - val_loss: 1.7505 - val_accuracy: 0.2705\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 98s 745ms/step - loss: 0.5385 - accuracy: 0.7313 - val_loss: 0.7285 - val_accuracy: 0.5610\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 90s 687ms/step - loss: 0.5195 - accuracy: 0.7387 - val_loss: 0.8392 - val_accuracy: 0.7457\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 92s 703ms/step - loss: 0.5184 - accuracy: 0.7438 - val_loss: 0.5355 - val_accuracy: 0.7524\n",
      "33/33 [==============================] - 16s 458ms/step\n",
      "F1 Score: 0.1875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from skimage.transform import resize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load your dataset and split it into training and testing sets\n",
    "# X and y represent your dataset features and labels, respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Resize the images to match the input shape of MobileNet\n",
    "X_train_resized = np.array([resize(img, (224, 224, 3)) for img in X_train])\n",
    "X_test_resized = np.array([resize(img, (224, 224, 3)) for img in X_test])\n",
    "\n",
    "# Preprocess the input data\n",
    "X_train_resized = preprocess_input(X_train_resized)\n",
    "X_test_resized = preprocess_input(X_test_resized)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train_resized)\n",
    "\n",
    "# Load the pre-trained MobileNet model without the top layers\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Create a new model on top of the pre-trained model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Freeze the layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data augmentation\n",
    "model.fit(\n",
    "    datagen.flow(X_train_resized, y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train_resized) // 32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_resized, y_test)\n",
    ")\n",
    "\n",
    "# Unfreeze the last few layers of the base model for fine-tuning\n",
    "for layer in base_model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Lower the learning rate for fine-tuning\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with fine-tuning\n",
    "model.fit(\n",
    "    datagen.flow(X_train_resized, y_train, batch_size=32),\n",
    "    steps_per_epoch=len(X_train_resized) // 32,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test_resized, y_test)\n",
    ")\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test_resized)\n",
    "y_pred = np.round(y_pred).flatten()\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d697da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
